{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10723, 5245)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import data_format as df\n",
    "gen_save_model = \"gen_batch_2622_full.h5\"\n",
    "disc_save_model = \"disc_batch_2622_full.h5\"\n",
    "noise_dim = 2622\n",
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 128\n",
    "data=np.load(\"origin_data.npy\")\n",
    "try:\n",
    "    noises=np.load('noise_2622.npy')\n",
    "except:\n",
    "    print(\"makeing_noise\")\n",
    "    noises=tf.random.normal([data.shape[0], noise_dim])\n",
    "    np.save('noise_2622.npy', noises)\n",
    "print(data.shape)\n",
    "dataset=tf.data.Dataset.from_tensor_slices((data,noises)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.23687503 0.61985046 0.38601926 ... 0.44107363 0.2208693  0.4277956 ]], shape=(1, 5245), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def build_generator():\n",
    "    inp = tf.keras.Input((noise_dim,))\n",
    "    x1 = tf.keras.layers.Dense(dense_dim/4, name=\"layer11\", bias_regularizer=tf.keras.regularizers.l2(0.01))(inp)\n",
    "    x1 = tf.keras.layers.Dense(dense_dim/2, name=\"layer12\", bias_regularizer=tf.keras.regularizers.l2(0.01))(x1)\n",
    "    x1 = tf.keras.layers.Dense(dense_dim/4, name=\"layer13\", bias_regularizer=tf.keras.regularizers.l2(0.01))(x1)\n",
    "    x1 = tf.keras.layers.BatchNormalization(name=\"bn11\")(x1)\n",
    "    x2 = tf.keras.layers.Dense(dense_dim/4, name=\"layer14\", bias_regularizer=tf.keras.regularizers.l2(0.01))(inp)\n",
    "    x2 = tf.keras.layers.BatchNormalization(name=\"bn12\")(x2)   \n",
    "    x = 0.5*x1 + 0.5*x2\n",
    "    \n",
    "    x1 = tf.keras.layers.Dense(dense_dim/4, name=\"layer21\", bias_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "    x1 = tf.keras.layers.Dense(dense_dim/2, name=\"layer22\", bias_regularizer=tf.keras.regularizers.l2(0.01))(x1)\n",
    "    x1 = tf.keras.layers.Dense(dense_dim/4, name=\"layer23\", bias_regularizer=tf.keras.regularizers.l2(0.01))(x1)\n",
    "    x1 = tf.keras.layers.BatchNormalization(name=\"bn21\")(x1)\n",
    "    x2 = tf.keras.layers.Dense(dense_dim/4, name=\"layer24\", bias_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "    x2 = tf.keras.layers.BatchNormalization(name=\"bn22\")(x2)    \n",
    "    x = 0.5*x1 + 0.5*x2\n",
    "    \n",
    "    x1 = tf.keras.layers.Dense(dense_dim/2, name=\"layer31\", bias_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "    x1 = tf.keras.layers.Dense(dense_dim, name=\"layer32\",bias_regularizer=tf.keras.regularizers.l2(0.01))(x1)\n",
    "    x1 = tf.keras.layers.Dense(dense_dim/2, name=\"layer33\", bias_regularizer=tf.keras.regularizers.l2(0.01))(x1)\n",
    "    x1 = tf.keras.layers.BatchNormalization(name=\"bn31\")(x1)\n",
    "    x2 = tf.keras.layers.Dense(dense_dim/2, name=\"layer34\", bias_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "    x2 = tf.keras.layers.BatchNormalization(name=\"bn32\")(x2)   \n",
    "    x = 0.5*x1 + 0.5*x2\n",
    "    \n",
    "    x1 = tf.keras.layers.Dense(dense_dim/2, name=\"layer41\", bias_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "    x1 = tf.keras.layers.Dense(dense_dim, name=\"layer42\", bias_regularizer=tf.keras.regularizers.l2(0.01))(x1)\n",
    "    x1 = tf.keras.layers.Dense(dense_dim/2, name=\"layer43\", bias_regularizer=tf.keras.regularizers.l2(0.01))(x1)\n",
    "    x1 = tf.keras.layers.BatchNormalization(name=\"bn41\")(x1)\n",
    "    x2 = tf.keras.layers.Dense(dense_dim/2, name=\"layer44\", bias_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "    x2 = tf.keras.layers.BatchNormalization(name=\"bn42\")(x2)   \n",
    "    x = 0.5*x1 + 0.5*x2\n",
    "    \n",
    "    x1 = tf.keras.layers.Dense(data.shape[-1], name=\"layer51\", bias_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "    x1 = tf.keras.layers.Dense(data.shape[-1]*2, name=\"layer52\", bias_regularizer=tf.keras.regularizers.l2(0.01))(x1)\n",
    "    x1 = tf.keras.layers.Dense(data.shape[-1], name=\"layer53\", bias_regularizer=tf.keras.regularizers.l2(0.01))(x1) \n",
    "    x1 = tf.keras.layers.BatchNormalization(name=\"bn51\")(x1)\n",
    "    x2 = tf.keras.layers.Dense(data.shape[-1], name=\"layer54\", bias_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "    x2 = tf.keras.layers.BatchNormalization(name=\"bn52\")(x2)      \n",
    "    x = 0.5*x1 + 0.5*x2\n",
    "    \n",
    "    x = tf.keras.layers.Attention(name=\"att1\")([x, x])\n",
    "    o = x\n",
    "    \n",
    "    return tf.keras.Model(inputs=[inp], outputs=[o])\n",
    "generator = build_generator()\n",
    "try:\n",
    "    generator.load_weights(gen_save_model, by_name=True)\n",
    "except:\n",
    "    print(\"erro loading pre_g\")\n",
    "\n",
    "noise = tf.random.normal([1, noise_dim])\n",
    "print(generator(noise, training=False))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-0.12248607]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def build_discriminator():\n",
    "    inp = tf.keras.Input(shape=(data.shape[-1],))\n",
    "    x = tf.keras.layers.Attention(name=\"att\")([inp, inp])\n",
    "    \n",
    "    x1 = tf.keras.layers.Dense(dense_dim, name=\"layer11\",bias_regularizer=tf.keras.regularizers.l2(0.01))(x)  \n",
    "    x1 = tf.keras.layers.Dense(dense_dim*2, name=\"layer12\",bias_regularizer=tf.keras.regularizers.l2(0.01))(x1)\n",
    "    x1 = tf.keras.layers.Dense(dense_dim, name=\"layer13\",bias_regularizer=tf.keras.regularizers.l2(0.01))(x1)   \n",
    "    x2 = tf.keras.layers.Dense(dense_dim, name=\"layer14\",bias_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "    x = 0.5*x1 + 0.5*x2\n",
    "    \n",
    "    x1 = tf.keras.layers.Dense(dense_dim/2, name=\"layer21\", bias_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "    x1 = tf.keras.layers.Dense(dense_dim, name=\"layer22\", bias_regularizer=tf.keras.regularizers.l2(0.01))(x1)\n",
    "    x1 = tf.keras.layers.Dense(dense_dim/2, name=\"layer23\", bias_regularizer=tf.keras.regularizers.l2(0.01))(x1)   \n",
    "    x2 = tf.keras.layers.Dense(dense_dim/2, name=\"layer24\", bias_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "    x = 0.5*x1 + 0.5*x2\n",
    "    \n",
    "    x1 = tf.keras.layers.Dense(dense_dim/4, name=\"layer31\", bias_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "    x1 = tf.keras.layers.Dense(dense_dim/2, name=\"layer32\", bias_regularizer=tf.keras.regularizers.l2(0.01))(x1)\n",
    "    x1 = tf.keras.layers.Dense(dense_dim/4, name=\"layer33\", bias_regularizer=tf.keras.regularizers.l2(0.01))(x1)\n",
    "    x2 = tf.keras.layers.Dense(dense_dim/4, name=\"layer34\", bias_regularizer=tf.keras.regularizers.l2(0.01))(x) \n",
    "    x = 0.5*x1 + 0.5*x2\n",
    "    \n",
    "    o = tf.keras.layers.Dense(1, name=\"layero\")(x)\n",
    "    return tf.keras.Model(inputs=[inp], outputs=[o])\n",
    "discriminator = build_discriminator()\n",
    "try:\n",
    "    discriminator.load_weights(disc_save_model, by_name=True)\n",
    "except:\n",
    "    print(\"erro loading pre_g\")\n",
    "print(discriminator(generator(noise, training=False),training=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "cross_entropy_numpy = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE,from_logits=True)\n",
    "\n",
    "def log(x):\n",
    "    return tf.math.log(x + 1e-8)\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "    \n",
    "def generator_loss(fake_output):\n",
    "    fake_loss1 = cross_entropy_numpy(tf.ones_like(fake_output), fake_output)\n",
    "    fake_loss2 = cross_entropy_numpy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = 0.5*tf.reduce_mean((fake_loss1 - fake_loss2)**2)\n",
    "    return total_loss\n",
    "\n",
    "generator_optimizer_low = tf.keras.optimizers.Adam(1e-8)\n",
    "generator_optimizer_high = tf.keras.optimizers.SGD(1e-4)\n",
    "discriminator_optimizer_high = tf.keras.optimizers.SGD(1e-8)\n",
    "discriminator_optimizer_low = tf.keras.optimizers.Adam(1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_g_low(images,noise):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    generator_optimizer_low.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    return gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_g_high(images,noise):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    generator_optimizer_high.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    return gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_all_high_low(images,noise):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer_high.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer_low.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    return gen_loss , disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_all_low_high(images,noise):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer_low.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer_high.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    return gen_loss , disc_loss ,real_output ,fake_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_all_high_high(images,noise):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer_high.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer_high.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    return gen_loss , disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_all_low_low(images,noise):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer_low.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer_low.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    return gen_loss , disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs, lenght=10, loss_range=3):\n",
    "    best_model_loss=[]\n",
    "    gen_loss_history = []\n",
    "    disc_loss_history = []\n",
    "    gen_opt = \"low\"\n",
    "    disc_opt = \"high\"\n",
    "    for epoch in range(epochs):\n",
    "        st = time.time()\n",
    "        for i, noise in dataset:\n",
    "            if(gen_opt == \"high\" and disc_opt == \"high\"):\n",
    "                _ = train_step_g_high(i,noise)\n",
    "                gen_loss, disc_loss = train_step_all_high_high(i, noise)\n",
    "            elif(gen_opt == \"low\" and disc_opt == \"low\"):\n",
    "                _ = train_step_g_low(i,noise)\n",
    "                gen_loss, disc_loss = train_step_all_low_low(i, noise)\n",
    "            elif(gen_opt == \"high\" and disc_opt == \"low\"):\n",
    "                _ = train_step_g_high(i,noise)\n",
    "                gen_loss, disc_loss = train_step_all_high_low(i, noise)\n",
    "            elif(gen_opt == \"low\" and disc_opt == \"high\"):\n",
    "                _ = train_step_g_low(i,noise)\n",
    "                gen_loss, disc_loss = train_step_all_low_high(i, noise)\n",
    "        disc_loss_ = float(disc_loss)\n",
    "        gen_loss_  = float(gen_loss)\n",
    "        if(len(disc_loss_history) < lenght):\n",
    "            disc_loss_history.append(disc_loss_)\n",
    "        else:\n",
    "            tmp = 0\n",
    "            for i in range(1,len(disc_loss_history)):\n",
    "                tmp+=abs(disc_loss_history[i-1]-disc_loss_history[i])\n",
    "            if(tmp <= loss_range):\n",
    "                print(\"disc lr = 1e-4\")\n",
    "                disc_opt = \"high\"\n",
    "                disc_loss_history = []\n",
    "            else:\n",
    "                print(\"disc lr = 1e-8\")\n",
    "                disc_opt = \"low\"\n",
    "                disc_loss_history = []\n",
    "        if(len(gen_loss_history) < lenght):\n",
    "            gen_loss_history.append(gen_loss_)\n",
    "        else:\n",
    "            tmp = 0\n",
    "            for i in range(1, len(gen_loss_history)):\n",
    "                tmp += abs(gen_loss_history[i-1]-gen_loss_history[i])\n",
    "            if(tmp <= loss_range):\n",
    "                print(\"gen lr = 1e-4\")\n",
    "                gen_opt = \"high\"\n",
    "                gen_loss_history = []\n",
    "            else:\n",
    "                print(\"gen lr = 1e-8\")\n",
    "                gen_opt = \"low\"\n",
    "                gen_loss_history = []\n",
    "        if(best_model_loss == [] or abs(best_model_loss[0] / best_model_loss[1] * disc_loss_) >= abs(gen_loss_)):\n",
    "            best_model_loss = [gen_loss_, disc_loss_]\n",
    "            generator.save_weights(gen_save_model)\n",
    "            discriminator.save_weights(disc_save_model)\n",
    "            print(\"epoch \", epoch, \" time:\", time.time() - st ,\"                           best_score\")\n",
    "        else:\n",
    "            print(\"epoch \", epoch, \" time:\", time.time() - st)\n",
    "        print(\"gen_loss:\", float(gen_loss), \" disc_loss:\", float(disc_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs=10000\n",
    "train(dataset,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
