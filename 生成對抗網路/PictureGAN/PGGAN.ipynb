{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, InputSpec, Conv2D, Conv2DTranspose, Activation, Reshape, LayerNormalization, BatchNormalization, UpSampling2D\n",
    "from tensorflow.keras.layers import Input, UpSampling2D, Dropout, Concatenate, Add, Dense, Multiply, LeakyReLU, Flatten, AveragePooling2D, Multiply\n",
    "from tensorflow.keras import initializers, regularizers, constraints, Model, Sequential\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'PGGAN'\n",
    "DATA_BASE_DIR = './flower_dataset/sunflower' # Modify this to your dataset path.\n",
    "OUTPUT_PATH = 'outputs2'\n",
    "MODEL_PATH = 'models2'\n",
    "TRAIN_LOGDIR = os.path.join(\"logs\", \"tensorflow\", 'train_data') # Sets up a log directory.\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "    \n",
    "batch_size = 1\n",
    "image_size = 4\n",
    "NOISE_DIM = 512\n",
    "LAMBDA = 1\n",
    "\n",
    "EPOCHs =720\n",
    "CURRENT_EPOCH = 1 \n",
    "total_data_number = len(os.listdir(DATA_BASE_DIR))\n",
    "\n",
    "switch_res_every_n_epoch = 120\n",
    "\n",
    "SAVE_EVERY_N_EPOCH = 5\n",
    "\n",
    "LR = 1e-3\n",
    "BETA_1 = 0.\n",
    "BETA_2 = 0.99\n",
    "EPSILON = 1e-8\n",
    "MIN_LR = 0.000001\n",
    "DECAY_FACTOR=1.00004\n",
    "\n",
    "file_writer = tf.summary.create_file_writer(TRAIN_LOGDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'./flower_dataset/sunflower/14925397761_46ecfa24e0.jpg'\n"
     ]
    }
   ],
   "source": [
    "list_ds = tf.data.Dataset.list_files(DATA_BASE_DIR + '/*')\n",
    "list_ds=list_ds.take(1)\n",
    "for f in list_ds.take(5):\n",
    "    print(f.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image - 127.5) / 127.5\n",
    "    return image\n",
    "\n",
    "def augmentation(image):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    return image\n",
    "\n",
    "def preprocess_image(file_path, target_size=512):\n",
    "    images = tf.io.read_file(file_path)\n",
    "    images = tf.image.decode_jpeg(images, channels=3)\n",
    "    images = tf.image.resize(images, (target_size, target_size),\n",
    "                           method='nearest', antialias=True)\n",
    "    images = augmentation(images)\n",
    "    images = normalize(images)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_function = partial(preprocess_image, target_size=image_size)\n",
    "train_data = list_ds.map(preprocess_function).shuffle(100).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fcfc425a160>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_img = next(iter(train_data))\n",
    "plt.title('Sample')\n",
    "plt.imshow(np.clip(sample_img[0] * 0.5 + 0.5, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EqualizeLearningRate(tf.keras.layers.Wrapper):\n",
    "    def __init__(self, layer, **kwargs):\n",
    "        super(EqualizeLearningRate, self).__init__(layer, **kwargs)\n",
    "        self._track_trackable(layer, name='layer')\n",
    "        self.is_rnn = isinstance(self.layer, tf.keras.layers.RNN)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_shape = tf.TensorShape(input_shape)\n",
    "        self.input_spec = tf.keras.layers.InputSpec(\n",
    "            shape=[None] + input_shape[1:])\n",
    "\n",
    "        if not self.layer.built:\n",
    "            self.layer.build(input_shape)\n",
    "\n",
    "        kernel_layer = self.layer.cell if self.is_rnn else self.layer\n",
    "\n",
    "        if not hasattr(kernel_layer, 'kernel'):\n",
    "            raise ValueError('`EqualizeLearningRate` must wrap a layer that'\n",
    "                             ' contains a `kernel` for weights')\n",
    "\n",
    "        if self.is_rnn:\n",
    "            kernel = kernel_layer.recurrent_kernel\n",
    "        else:\n",
    "            kernel = kernel_layer.kernel\n",
    "\n",
    "        self.fan_in, self.fan_out= self._compute_fans(kernel.shape)\n",
    "        self.he_constant = tf.Variable(1.0 / np.sqrt(self.fan_in), dtype=tf.float32, trainable=False)\n",
    "\n",
    "        self.v = kernel\n",
    "        self.built = True\n",
    "    \n",
    "    def call(self, inputs, training=True):\n",
    "        with tf.name_scope('compute_weights'):\n",
    "            kernel = tf.identity(self.v * self.he_constant)\n",
    "            \n",
    "            if self.is_rnn:\n",
    "                print(self.is_rnn)\n",
    "                self.layer.cell.recurrent_kernel = kernel\n",
    "                update_kernel = tf.identity(self.layer.cell.recurrent_kernel)\n",
    "            else:\n",
    "                self.layer.kernel = kernel\n",
    "                update_kernel = tf.identity(self.layer.kernel)\n",
    "\n",
    "            with tf.control_dependencies([update_kernel]):\n",
    "                outputs = self.layer(inputs)\n",
    "                return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tf.TensorShape(\n",
    "            self.layer.compute_output_shape(input_shape).as_list())\n",
    "    \n",
    "    def _compute_fans(self, shape, data_format='channels_last'):\n",
    "        if len(shape) == 2:\n",
    "            fan_in = shape[0]\n",
    "            fan_out = shape[1]\n",
    "        elif len(shape) in {3, 4, 5}:\n",
    "            if data_format == 'channels_first':\n",
    "                receptive_field_size = np.prod(shape[2:])\n",
    "                fan_in = shape[1] * receptive_field_size\n",
    "                fan_out = shape[0] * receptive_field_size\n",
    "            elif data_format == 'channels_last':\n",
    "                receptive_field_size = np.prod(shape[:-2])\n",
    "                fan_in = shape[-2] * receptive_field_size\n",
    "                fan_out = shape[-1] * receptive_field_size\n",
    "            else:\n",
    "                raise ValueError('Invalid data_format: ' + data_format)\n",
    "        else:\n",
    "            fan_in = np.sqrt(np.prod(shape))\n",
    "            fan_out = np.sqrt(np.prod(shape))\n",
    "        return fan_in, fan_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_initializer = 'he_normal'\n",
    "\n",
    "class PixelNormalization(tf.keras.layers.Layer):\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        super(PixelNormalization, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs / tf.sqrt(tf.reduce_mean(tf.square(inputs), axis=-1, keepdims=True) + self.epsilon)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "class MinibatchSTDDEV(tf.keras.layers.Layer):\n",
    "    def __init__(self, group_size=4):\n",
    "        super(MinibatchSTDDEV, self).__init__()\n",
    "        self.group_size = group_size\n",
    "\n",
    "    def call(self, inputs):\n",
    "        group_size = tf.minimum(self.group_size, tf.shape(inputs)[0])     # Minibatch must be divisible by (or smaller than) group_size.\n",
    "        s = inputs.shape                                             # [NHWC]  Input shape.\n",
    "        y = tf.reshape(inputs, [group_size, -1, s[1], s[2], s[3]])   # [GMHWC] Split minibatch into M groups of size G.\n",
    "        y = tf.cast(y, tf.float32)                              # [GMHWC] Cast to FP32.\n",
    "        y -= tf.reduce_mean(y, axis=0, keepdims=True)           # [GMHWC] Subtract mean over group.\n",
    "        y = tf.reduce_mean(tf.square(y), axis=0)                # [MHWC]  Calc variance over group.\n",
    "        y = tf.sqrt(y + 1e-8)                                   # [MHWC]  Calc stddev over group.\n",
    "        y = tf.reduce_mean(y, axis=[1,2,3], keepdims=True)      # [M111]  Take average over fmaps and pixels.\n",
    "        y = tf.cast(y, inputs.dtype)                                 # [M111]  Cast back to original data type.\n",
    "        y = tf.tile(y, [group_size, s[1], s[2], 1])             # [NHW1]  Replicate over group and pixels.\n",
    "        return tf.concat([inputs, y], axis=-1)                        # [NHWC]  Append as new fmap.\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], input_shape[2], input_shape[3] + 1)\n",
    "\n",
    "\n",
    "def upsample_block(x, in_filters, filters, kernel_size=3, strides=1, padding='valid', activation=tf.nn.leaky_relu, name=''):\n",
    "    upsample = UpSampling2D(size=2, interpolation='nearest')(x)\n",
    "    upsample_x = EqualizeLearningRate(Conv2D(filters, kernel_size, strides, padding=padding,\n",
    "                   kernel_initializer=kernel_initializer, bias_initializer='zeros'), name=name+'_conv2d_1')(upsample)\n",
    "    x = PixelNormalization()(upsample_x)\n",
    "    x = Activation(activation)(x)\n",
    "    x = EqualizeLearningRate(Conv2D(filters, kernel_size, strides, padding=padding,\n",
    "                                   kernel_initializer=kernel_initializer, bias_initializer='zeros'), name=name+'_conv2d_2')(x)\n",
    "    x = PixelNormalization()(x)\n",
    "    x = Activation(activation)(x)\n",
    "    return x, upsample\n",
    "\n",
    "def downsample_block(x, filters1, filters2, kernel_size=3, strides=1, padding='valid', activation=tf.nn.leaky_relu, name=''):\n",
    "    x = EqualizeLearningRate(Conv2D(filters1, kernel_size, strides, padding=padding,\n",
    "               kernel_initializer=kernel_initializer, bias_initializer='zeros'), name=name+'_conv2d_1')(x)\n",
    "    x = Activation(activation)(x)\n",
    "    x = EqualizeLearningRate(Conv2D(filters2, kernel_size, strides, padding=padding,\n",
    "               kernel_initializer=kernel_initializer, bias_initializer='zeros'), name=name+'_conv2d_2')(x)\n",
    "    x = Activation(activation)(x)\n",
    "    downsample = AveragePooling2D(pool_size=2)(x)\n",
    "\n",
    "    return downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_activation = tf.keras.activations.tanh\n",
    "\n",
    "def generator_input_block(x):\n",
    "    x = EqualizeLearningRate(Dense(4*4*512, kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='g_input_dense')(x)\n",
    "    x = PixelNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Reshape((4, 4, 512))(x)\n",
    "    x = EqualizeLearningRate(Conv2D(512, 3, strides=1, padding='same',\n",
    "                                          kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='g_input_conv2d')(x)\n",
    "    x = PixelNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    return x\n",
    "\n",
    "def build_4x4_generator(noise_dim=NOISE_DIM):\n",
    "    inputs = Input(noise_dim)\n",
    "    x = generator_input_block(inputs)\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(4, 4))\n",
    "    \n",
    "    rgb_out = to_rgb(x)\n",
    "    model = Model(inputs=[inputs, alpha], outputs=rgb_out)\n",
    "    return model\n",
    "\n",
    "def build_8x8_generator(noise_dim=NOISE_DIM):\n",
    "    inputs = Input(noise_dim)\n",
    "    x = generator_input_block(inputs)\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    x, up_x = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(8, 8))\n",
    "    \n",
    "    \n",
    "    previous_to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(4, 4))\n",
    "    to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(8, 8))\n",
    "\n",
    "    l_x = to_rgb(x)\n",
    "    r_x = previous_to_rgb(up_x)\n",
    "    l_x = Multiply()([1 - alpha, l_x])\n",
    "    r_x = Multiply()([alpha, r_x])\n",
    "    combined = Add()([l_x, r_x])\n",
    "    \n",
    "    model = Model(inputs=[inputs, alpha], outputs=combined)\n",
    "    return model\n",
    "\n",
    "def build_16x16_generator(noise_dim=NOISE_DIM):\n",
    "    inputs = Input(noise_dim)\n",
    "    x = generator_input_block(inputs)\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    x, _ = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(8, 8))\n",
    "    x, up_x = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(16, 16))\n",
    "    \n",
    "    previous_to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(8, 8))\n",
    "    to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(16, 16))\n",
    "\n",
    "    l_x = to_rgb(x)\n",
    "    r_x = previous_to_rgb(up_x)\n",
    "    l_x = Multiply()([1 - alpha, l_x])\n",
    "    r_x = Multiply()([alpha, r_x])\n",
    "    combined = Add()([l_x, r_x])\n",
    "    \n",
    "    model = Model(inputs=[inputs, alpha], outputs=combined)\n",
    "    return model\n",
    "\n",
    "def build_32x32_generator(noise_dim=NOISE_DIM):\n",
    "    inputs = Input(noise_dim)\n",
    "    x = generator_input_block(inputs)\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    x, _ = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(8, 8))\n",
    "    x, _ = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(16, 16))\n",
    "    x, up_x = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(32, 32))\n",
    "    \n",
    "    previous_to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(16, 16))\n",
    "    to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(32, 32))\n",
    "\n",
    "    l_x = to_rgb(x)\n",
    "    r_x = previous_to_rgb(up_x)\n",
    "    l_x = Multiply()([1 - alpha, l_x])\n",
    "    r_x = Multiply()([alpha, r_x])\n",
    "    combined = Add()([l_x, r_x])\n",
    "    \n",
    "    model = Model(inputs=[inputs, alpha], outputs=combined)\n",
    "    return model\n",
    "\n",
    "def build_64x64_generator(noise_dim=NOISE_DIM):\n",
    "    inputs = Input(noise_dim)\n",
    "    x = generator_input_block(inputs)\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    x, _ = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(8, 8))\n",
    "    x, _ = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(16, 16))\n",
    "    x, _ = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(32, 32))\n",
    "    x, up_x = upsample_block(x, in_filters=512, filters=256, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(64, 64))\n",
    "    \n",
    "    previous_to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(32, 32))\n",
    "    to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(64, 64))\n",
    "    \n",
    "    l_x = to_rgb(x)\n",
    "    r_x = previous_to_rgb(up_x)\n",
    "    l_x = Multiply()([1 - alpha, l_x])\n",
    "    r_x = Multiply()([alpha, r_x])\n",
    "    combined = Add()([l_x, r_x])\n",
    "    \n",
    "    model = Model(inputs=[inputs, alpha], outputs=combined)\n",
    "    return model\n",
    "\n",
    "def build_128x128_generator(noise_dim=NOISE_DIM):\n",
    "    inputs = Input(noise_dim)\n",
    "    x = generator_input_block(inputs)\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    x, _ = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(8, 8))\n",
    "    x, _ = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(16, 16))\n",
    "    x, _ = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(32, 32))\n",
    "    x, _ = upsample_block(x, in_filters=512, filters=256, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(64, 64))\n",
    "    x, up_x = upsample_block(x, in_filters=256, filters=128, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(128, 128))\n",
    "    \n",
    "    previous_to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(64, 64))\n",
    "    to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(128, 128))\n",
    "    \n",
    "    l_x = to_rgb(x)\n",
    "    r_x = previous_to_rgb(up_x)\n",
    "    l_x = Multiply()([1 - alpha, l_x])\n",
    "    r_x = Multiply()([alpha, r_x])\n",
    "    combined = Add()([l_x, r_x])\n",
    "    \n",
    "    model = Model(inputs=[inputs, alpha], outputs=combined)\n",
    "    return model\n",
    "\n",
    "def build_256x256_generator(noise_dim=NOISE_DIM):\n",
    "    inputs = Input(noise_dim)\n",
    "    x = generator_input_block(inputs)\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    x, _ = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(8, 8))\n",
    "    x, _ = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(16, 16))\n",
    "    x, _ = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(32, 32))\n",
    "    x, _ = upsample_block(x, in_filters=512, filters=256, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(64, 64))\n",
    "    x, _ = upsample_block(x, in_filters=256, filters=128, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(128, 128))\n",
    "    x, up_x = upsample_block(x, in_filters=128, filters=64, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(256, 256))\n",
    "    \n",
    "    previous_to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(128, 128))\n",
    "    to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(256, 256))\n",
    "    \n",
    "    l_x = to_rgb(x)\n",
    "    r_x = previous_to_rgb(up_x)\n",
    "    l_x = Multiply()([1 - alpha, l_x])\n",
    "    r_x = Multiply()([alpha, r_x])\n",
    "    combined = Add()([l_x, r_x])\n",
    "    \n",
    "    model = Model(inputs=[inputs, alpha], outputs=combined)\n",
    "    return model\n",
    "\n",
    "def build_512x512_generator(noise_dim=NOISE_DIM):\n",
    "    inputs = Input(noise_dim)\n",
    "    x = generator_input_block(inputs)\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    x, _ = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(8, 8))\n",
    "    x, _ = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(16, 16))\n",
    "    x, _ = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(32, 32))\n",
    "    x, _ = upsample_block(x, in_filters=512, filters=256, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(64, 64))\n",
    "    x, _ = upsample_block(x, in_filters=256, filters=128, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(128, 128))\n",
    "    x, _ = upsample_block(x, in_filters=128, filters=64, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(256, 256))\n",
    "    x, up_x = upsample_block(x, in_filters=64, filters=32, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(512, 512))\n",
    "    \n",
    "    previous_to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(256, 256))\n",
    "    to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(512, 512))\n",
    "    \n",
    "    l_x = to_rgb(x)\n",
    "    r_x = previous_to_rgb(up_x)\n",
    "    l_x = Multiply()([1 - alpha, l_x])\n",
    "    r_x = Multiply()([alpha, r_x])\n",
    "    combined = Add()([l_x, r_x])\n",
    "    \n",
    "    model = Model(inputs=[inputs, alpha], outputs=combined)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_block(x):\n",
    "    x = MinibatchSTDDEV()(x)\n",
    "    x = EqualizeLearningRate(Conv2D(512, 3, strides=1, padding='same',\n",
    "                                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='d_output_conv2d_1')(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = EqualizeLearningRate(Conv2D(512, 4, strides=1, padding='valid',\n",
    "                                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='d_output_conv2d_2')(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = EqualizeLearningRate(Dense(1, kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='d_output_dense')(x)\n",
    "    return x\n",
    "\n",
    "def build_4x4_discriminator():\n",
    "    inputs = Input((4,4,3))\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    from_rgb = EqualizeLearningRate(Conv2D(512, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(4, 4))\n",
    "    x = from_rgb(inputs)\n",
    "    x = EqualizeLearningRate(Conv2D(512, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='conv2d_up_channel')(x)\n",
    "    x = discriminator_block(x)\n",
    "    model = Model(inputs=[inputs, alpha], outputs=x)\n",
    "    return model\n",
    "\n",
    "def build_8x8_discriminator():\n",
    "    fade_in_channel = 512\n",
    "    inputs = Input((8,8,3))\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    downsample = AveragePooling2D(pool_size=2)\n",
    "    previous_from_rgb = EqualizeLearningRate(Conv2D(512, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(4, 4))\n",
    "    l_x = previous_from_rgb(downsample(inputs))\n",
    "    l_x = Multiply()([1 - alpha, l_x])\n",
    "    from_rgb = EqualizeLearningRate(Conv2D(512, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(8, 8))\n",
    "    r_x = from_rgb(inputs)\n",
    "    r_x = downsample_block(r_x, filters1=512, filters2=fade_in_channel, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(8,8))\n",
    "    r_x = Multiply()([alpha, r_x])\n",
    "    x = Add()([l_x, r_x])\n",
    "    x = discriminator_block(x)\n",
    "    model = Model(inputs=[inputs, alpha], outputs=x)\n",
    "    return model\n",
    "\n",
    "def build_16x16_discriminator():\n",
    "    fade_in_channel = 512\n",
    "    inputs = Input((16, 16, 3))\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    downsample = AveragePooling2D(pool_size=2)\n",
    "    previous_from_rgb = EqualizeLearningRate(Conv2D(512, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(8, 8))\n",
    "    l_x = previous_from_rgb(downsample(inputs))\n",
    "    l_x = Multiply()([1 - alpha, l_x])\n",
    "    from_rgb = EqualizeLearningRate(Conv2D(512, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(16, 16))\n",
    "    r_x = from_rgb(inputs)\n",
    "    r_x = downsample_block(r_x, filters1=512, filters2=fade_in_channel, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(16,16))\n",
    "    r_x = Multiply()([alpha, r_x])\n",
    "    x = Add()([l_x, r_x])\n",
    "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(8,8))\n",
    "    x = discriminator_block(x)\n",
    "    model = Model(inputs=[inputs, alpha], outputs=x)\n",
    "    return model\n",
    "\n",
    "def build_32x32_discriminator():\n",
    "    fade_in_channel = 512\n",
    "    inputs = Input((32, 32, 3))\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    downsample = AveragePooling2D(pool_size=2)\n",
    "    previous_from_rgb = EqualizeLearningRate(Conv2D(512, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(16, 16))\n",
    "    l_x = previous_from_rgb(downsample(inputs))\n",
    "    l_x = Multiply()([1 - alpha, l_x])\n",
    "    from_rgb = EqualizeLearningRate(Conv2D(512, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(32, 32))\n",
    "    r_x = from_rgb(inputs)\n",
    "    r_x = downsample_block(r_x, filters1=512, filters2=fade_in_channel, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(32,32))\n",
    "    r_x = Multiply()([alpha, r_x])\n",
    "    x = Add()([l_x, r_x])\n",
    "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(16,16))\n",
    "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(8,8))\n",
    "    x = discriminator_block(x)\n",
    "    model = Model(inputs=[inputs, alpha], outputs=x)\n",
    "    return model\n",
    "\n",
    "def build_64x64_discriminator():\n",
    "    fade_in_channel = 512\n",
    "    inputs = Input((64, 64, 3))\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    downsample = AveragePooling2D(pool_size=2)\n",
    "    previous_from_rgb = EqualizeLearningRate(Conv2D(512, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(32, 32))\n",
    "    l_x = previous_from_rgb(downsample(inputs))\n",
    "    l_x = Multiply()([1 - alpha, l_x])\n",
    "    from_rgb = EqualizeLearningRate(Conv2D(256, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(64, 64))\n",
    "    r_x = from_rgb(inputs)\n",
    "    r_x = downsample_block(r_x, filters1=256, filters2=fade_in_channel, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(64,64))\n",
    "    r_x = Multiply()([alpha, r_x])\n",
    "    x = Add()([l_x, r_x])\n",
    "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(32,32))\n",
    "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(16,16))\n",
    "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(8,8))\n",
    "    x = discriminator_block(x)\n",
    "    model = Model(inputs=[inputs, alpha], outputs=x)\n",
    "    return model\n",
    "\n",
    "def build_128x128_discriminator():\n",
    "    fade_in_channel = 256\n",
    "    inputs = Input((128, 128, 3))\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    downsample = AveragePooling2D(pool_size=2)\n",
    "    previous_from_rgb = EqualizeLearningRate(Conv2D(256, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(64, 64))\n",
    "    l_x = previous_from_rgb(downsample(inputs))\n",
    "    l_x = Multiply()([1 - alpha, l_x])\n",
    "    from_rgb = EqualizeLearningRate(Conv2D(128, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(128, 128))\n",
    "    r_x = from_rgb(inputs)\n",
    "    r_x = downsample_block(r_x, filters1=128, filters2=fade_in_channel, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(128,128))\n",
    "    r_x = Multiply()([alpha, r_x])\n",
    "    x = Add()([l_x, r_x])\n",
    "    x = downsample_block(x, filters1=256, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(64,64))\n",
    "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(32,32))\n",
    "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(16,16))\n",
    "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(8,8))\n",
    "    x = discriminator_block(x)\n",
    "    model = Model(inputs=[inputs, alpha], outputs=x)\n",
    "    return model\n",
    "\n",
    "def build_256x256_discriminator():\n",
    "    fade_in_channel = 128\n",
    "    inputs = Input((256, 256, 3))\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    downsample = AveragePooling2D(pool_size=2)\n",
    "    previous_from_rgb = EqualizeLearningRate(Conv2D(128, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(128, 128))\n",
    "    l_x = previous_from_rgb(downsample(inputs))\n",
    "    l_x = Multiply()([1 - alpha, l_x])\n",
    "    from_rgb = EqualizeLearningRate(Conv2D(64, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(256, 256))\n",
    "    r_x = from_rgb(inputs)\n",
    "    r_x = downsample_block(r_x, filters1=64, filters2=fade_in_channel, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(256,256))\n",
    "    r_x = Multiply()([alpha, r_x])\n",
    "    x = Add()([l_x, r_x])\n",
    "    x = downsample_block(x, filters1=128, filters2=256, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(128,128))\n",
    "    x = downsample_block(x, filters1=256, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(64,64))\n",
    "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(32,32))\n",
    "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(16,16))\n",
    "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(8,8))\n",
    "    x = discriminator_block(x)\n",
    "    model = Model(inputs=[inputs, alpha], outputs=x)\n",
    "    return model\n",
    "\n",
    "def build_512x512_discriminator():\n",
    "    fade_in_channel = 64\n",
    "    inputs = Input((512, 512, 3))\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    downsample = AveragePooling2D(pool_size=2)\n",
    "    \n",
    "    previous_from_rgb = EqualizeLearningRate(Conv2D(64, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(256, 256))\n",
    "    l_x = previous_from_rgb(downsample(inputs))\n",
    "    l_x = Multiply()([1 - alpha, l_x])\n",
    "    from_rgb = EqualizeLearningRate(Conv2D(32, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(512, 512))\n",
    "    r_x = from_rgb(inputs)\n",
    "    r_x = downsample_block(r_x, filters1=32, filters2=fade_in_channel, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(512,512))\n",
    "    r_x = Multiply()([alpha, r_x])\n",
    "    x = Add()([l_x, r_x])\n",
    "    x = downsample_block(x, filters1=64, filters2=128, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(256,256))\n",
    "    x = downsample_block(x, filters1=128, filters2=256, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(128,128))\n",
    "    x = downsample_block(x, filters1=256, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(64,64))\n",
    "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(32,32))\n",
    "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(16,16))\n",
    "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(8,8))\n",
    "    x = discriminator_block(x)\n",
    "    model = Model(inputs=[inputs, alpha], outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(target_resolution):\n",
    "    generator = None\n",
    "    discriminator = None\n",
    "    if target_resolution == 4:\n",
    "        generator = build_4x4_generator()\n",
    "        discriminator = build_4x4_discriminator()\n",
    "    elif target_resolution == 8:\n",
    "        generator = build_8x8_generator()\n",
    "        discriminator = build_8x8_discriminator()\n",
    "    elif target_resolution == 16:\n",
    "        generator = build_16x16_generator()\n",
    "        discriminator = build_16x16_discriminator()\n",
    "    elif target_resolution == 32:\n",
    "        generator = build_32x32_generator()\n",
    "        discriminator = build_32x32_discriminator()\n",
    "    elif target_resolution == 64:\n",
    "        generator = build_64x64_generator()\n",
    "        discriminator = build_64x64_discriminator()\n",
    "    elif target_resolution == 128:\n",
    "        generator = build_128x128_generator()\n",
    "        discriminator = build_128x128_discriminator()\n",
    "    elif target_resolution == 256:\n",
    "        generator = build_256x256_generator()\n",
    "        discriminator = build_256x256_discriminator()\n",
    "    elif target_resolution == 512:\n",
    "        generator = build_512x512_generator()\n",
    "        discriminator = build_512x512_discriminator()\n",
    "    else:\n",
    "        print(\"target resolution models are not defined yet\")\n",
    "    return generator, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "g_input_dense (EqualizeLearning (None, 8192)         4202497     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pixel_normalization (PixelNorma (None, 8192)         0           g_input_dense[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 8192)         0           pixel_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 4, 4, 512)    0           leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "g_input_conv2d (EqualizeLearnin (None, 4, 4, 512)    2359809     reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "pixel_normalization_1 (PixelNor (None, 4, 4, 512)    0           g_input_conv2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 4, 4, 512)    0           pixel_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "input_alpha (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "to_rgb_4x4 (EqualizeLearningRat (None, 4, 4, 3)      1540        leaky_re_lu_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 6,563,846\n",
      "Trainable params: 6,563,843\n",
      "Non-trainable params: 3\n",
      "__________________________________________________________________________________________________\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "generator, discriminator = model_builder(image_size)\n",
    "generator.summary()\n",
    "plot_model(generator, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 4, 4, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "from_rgb_4x4 (EqualizeLearningR (None, 4, 4, 512)    2049        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_up_channel (EqualizeLear (None, 4, 4, 512)    262657      from_rgb_4x4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "minibatch_stddev (MinibatchSTDD (None, 4, 4, 513)    0           conv2d_up_channel[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "d_output_conv2d_1 (EqualizeLear (None, 4, 4, 512)    2364417     minibatch_stddev[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 4, 4, 512)    0           d_output_conv2d_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "d_output_conv2d_2 (EqualizeLear (None, 1, 1, 512)    4194817     leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 1, 1, 512)    0           d_output_conv2d_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_alpha (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "d_output_dense (EqualizeLearnin (None, 1)            514         flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,824,454\n",
      "Trainable params: 6,824,449\n",
      "Non-trainable params: 5\n",
      "__________________________________________________________________________________________________\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()\n",
    "plot_model(discriminator, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_optimizer = Adam(learning_rate=LR, beta_1=BETA_1, beta_2=BETA_2, epsilon=EPSILON)\n",
    "G_optimizer = Adam(learning_rate=LR, beta_1=BETA_1, beta_2=BETA_2, epsilon=EPSILON)\n",
    "\n",
    "def learning_rate_decay(current_lr, decay_factor=DECAY_FACTOR):\n",
    "    new_lr = max(current_lr / decay_factor, MIN_LR)\n",
    "    return new_lr\n",
    "\n",
    "def set_learning_rate(new_lr, D_optimizer, G_optimizer):\n",
    "    K.set_value(D_optimizer.lr, new_lr)\n",
    "    K.set_value(G_optimizer.lr, new_lr)\n",
    "    \n",
    "def calculate_batch_size(image_size):\n",
    "    if image_size < 64:\n",
    "        return 1\n",
    "    elif image_size < 128:\n",
    "        return 1\n",
    "    elif image_size == 128:\n",
    "        return 1\n",
    "    elif image_size == 256:\n",
    "        return 1\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input, figure_size=(12,6), subplot=(3,6), save=True, is_flatten=False):\n",
    "    predictions = model.predict(test_input)\n",
    "    fig = plt.figure(figsize=figure_size)\n",
    "    for i in range(predictions.shape[0]):\n",
    "        axs = plt.subplot(subplot[0], subplot[1], i+1)\n",
    "        plt.imshow(predictions[i] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "    if save:\n",
    "        plt.savefig(os.path.join(OUTPUT_PATH, '{}x{}_image_at_epoch_{:04d}.png'.format(predictions.shape[1], predictions.shape[2], epoch)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_examples_to_generate = 9\n",
    "sample_noise = tf.random.normal([num_examples_to_generate, NOISE_DIM], seed=0)\n",
    "sample_alpha = np.repeat(1, num_examples_to_generate).reshape(num_examples_to_generate, 1).astype(np.float32)\n",
    "generate_and_save_images(generator, 0, [sample_noise, sample_alpha], figure_size=(6,6), subplot=(3,3), save=False, is_flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def WGAN_GP_train_d_step(generator, discriminator, real_image, alpha, batch_size, step):\n",
    "    noise = tf.random.normal([batch_size, NOISE_DIM])\n",
    "    epsilon = tf.random.uniform(shape=[batch_size, 1, 1, 1], minval=0, maxval=1)\n",
    "    with tf.GradientTape(persistent=True) as d_tape:\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            fake_image = generator([noise, alpha], training=True)\n",
    "            fake_image_mixed = epsilon * tf.dtypes.cast(real_image, tf.float32) + ((1 - epsilon) * fake_image)\n",
    "            fake_mixed_pred = discriminator([fake_image_mixed, alpha], training=True)\n",
    "            \n",
    "        grads = gp_tape.gradient(fake_mixed_pred, fake_image_mixed)\n",
    "        grad_norms = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "        gradient_penalty = tf.reduce_mean(tf.square(grad_norms - 1))\n",
    "        \n",
    "        fake_pred = discriminator([fake_image, alpha], training=True)\n",
    "        real_pred = discriminator([real_image, alpha], training=True)\n",
    "        \n",
    "        D_loss = tf.reduce_mean(fake_pred) - tf.reduce_mean(real_pred) + LAMBDA * gradient_penalty\n",
    "    D_gradients = d_tape.gradient(D_loss,\n",
    "                                            discriminator.trainable_variables)\n",
    "    D_optimizer.apply_gradients(zip(D_gradients,\n",
    "                                                discriminator.trainable_variables))\n",
    "    if step % 10 == 0:\n",
    "        with file_writer.as_default():\n",
    "            tf.summary.scalar('D_loss', tf.reduce_mean(D_loss), step=step)\n",
    "            \n",
    "@tf.function\n",
    "def WGAN_GP_train_g_step(generator, discriminator, alpha, batch_size, step):\n",
    "    noise = tf.random.normal([batch_size, NOISE_DIM])\n",
    "    with tf.GradientTape() as g_tape:\n",
    "        fake_image = generator([noise, alpha], training=True)\n",
    "\n",
    "        fake_pred = discriminator([fake_image, alpha], training=True)\n",
    "        G_loss = -tf.reduce_mean(fake_pred)\n",
    "    G_gradients = g_tape.gradient(G_loss,\n",
    "                                            generator.trainable_variables)\n",
    "    G_optimizer.apply_gradients(zip(G_gradients,\n",
    "                                                generator.trainable_variables))\n",
    "    if step % 10 == 0:\n",
    "        with file_writer.as_default():\n",
    "            tf.summary.scalar('G_loss', G_loss, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "734"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.ceil(total_data_number / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if image_size > 4:\n",
    "    if os.path.isfile(os.path.join(MODEL_PATH, '{}x{}_generator.h5'.format(int(image_size / 2), int(image_size / 2)))):\n",
    "        generator.load_weights(os.path.join(MODEL_PATH, '{}x{}_generator.h5'.format(int(image_size / 2), int(image_size / 2))), by_name=True)\n",
    "        print(\"generator loaded\")\n",
    "    if os.path.isfile(os.path.join(MODEL_PATH, '{}x{}_discriminator.h5'.format(int(image_size / 2), int(image_size / 2)))):\n",
    "        discriminator.load_weights(os.path.join(MODEL_PATH, '{}x{}_discriminator.h5'.format(int(image_size / 2), int(image_size / 2))), by_name=True)\n",
    "        print(\"discriminator loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(os.path.join(MODEL_PATH, '{}x{}_generator.h5'.format(int(image_size), int(image_size)))):\n",
    "    generator.load_weights(os.path.join(MODEL_PATH, '{}x{}_generator.h5'.format(int(image_size), int(image_size))), by_name=False)\n",
    "    print(\"generator loaded\")\n",
    "if os.path.isfile(os.path.join(MODEL_PATH, '{}x{}_discriminator.h5'.format(int(image_size), int(image_size)))):\n",
    "    discriminator.load_weights(os.path.join(MODEL_PATH, '{}x{}_discriminator.h5'.format(int(image_size), int(image_size))), by_name=False)\n",
    "    print(\"discriminator loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for epoch 478 is 0.9159281253814697 sec\n",
      "\n",
      "Start of epoch 479\n",
      "Current alpha: 1.000000\n",
      "Current resolution: 32 * 32\n"
     ]
    }
   ],
   "source": [
    "current_learning_rate = LR\n",
    "training_steps = math.ceil(total_data_number / batch_size)\n",
    "alpha_increment = 1. / (switch_res_every_n_epoch / 2 * training_steps)\n",
    "alpha =min(1., (CURRENT_EPOCH - 1) % switch_res_every_n_epoch * training_steps *  alpha_increment)\n",
    "\n",
    "for epoch in range(CURRENT_EPOCH, EPOCHs + 1):\n",
    "    start = time.time()\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "    print('Current alpha: %f' % (alpha,))\n",
    "    print('Current resolution: {} * {}'.format(image_size, image_size))\n",
    "   \n",
    "    for step, (image) in enumerate(train_data):\n",
    "        current_batch_size = image.shape[0]\n",
    "        alpha_tensor = tf.constant(np.repeat(alpha, current_batch_size).reshape(current_batch_size, 1), dtype=tf.float32)\n",
    "        \n",
    "        WGAN_GP_train_d_step(generator, discriminator, image, alpha_tensor,batch_size=tf.constant(current_batch_size, dtype=tf.int64), step=tf.constant(step, dtype=tf.int64))\n",
    "        WGAN_GP_train_g_step(generator, discriminator, alpha_tensor,batch_size=tf.constant(current_batch_size, dtype=tf.int64), step=tf.constant(step, dtype=tf.int64))\n",
    "        \n",
    "        \n",
    "        alpha = min(1., alpha + alpha_increment)\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            print ('.', end='')\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    generate_and_save_images(generator, epoch, [sample_noise, sample_alpha], figure_size=(6,6), subplot=(3,3), save=True, is_flatten=False)\n",
    "    \n",
    "    if epoch % SAVE_EVERY_N_EPOCH == 0:\n",
    "        generator.save_weights(os.path.join(MODEL_PATH, '{}x{}_generator.h5'.format(image_size, image_size)))\n",
    "        discriminator.save_weights(os.path.join(MODEL_PATH, '{}x{}_discriminator.h5'.format(image_size, image_size)))\n",
    "        print ('Saving model for epoch {}'.format(epoch))\n",
    "    \n",
    "    print ('Time taken for epoch {} is {} sec\\n'.format(epoch,\n",
    "                                                      time.time()-start))\n",
    "    \n",
    "    \n",
    "    if epoch % switch_res_every_n_epoch == 0:\n",
    "        print('saving {} * {} model'.format(image_size, image_size))\n",
    "        generator.save_weights(os.path.join(MODEL_PATH, '{}x{}_generator.h5'.format(image_size, image_size)))\n",
    "        discriminator.save_weights(os.path.join(MODEL_PATH, '{}x{}_discriminator.h5'.format(image_size, image_size)))\n",
    "        alpha = 0\n",
    "        previous_image_size = int(image_size)\n",
    "        image_size = int(image_size * 2)\n",
    "        if image_size > 512:\n",
    "            print('Resolution reach 512x512, finish training')\n",
    "            break\n",
    "        print('creating {} * {} model'.format(image_size, image_size))\n",
    "        generator, discriminator = model_builder(image_size)\n",
    "        generator.load_weights(os.path.join(MODEL_PATH, '{}x{}_generator.h5'.format(previous_image_size, previous_image_size)), by_name=True)\n",
    "        discriminator.load_weights(os.path.join(MODEL_PATH, '{}x{}_discriminator.h5'.format(previous_image_size, previous_image_size)), by_name=True)\n",
    "        \n",
    "        print('Making {} * {} dataset'.format(image_size, image_size))\n",
    "        batch_size = calculate_batch_size(image_size)\n",
    "        preprocess_function = partial(preprocess_image, target_size=image_size)\n",
    "        train_data = list_ds.map(preprocess_function).shuffle(100).batch(batch_size)\n",
    "        training_steps = math.ceil(total_data_number / batch_size)\n",
    "        alpha_increment = 1. / (switch_res_every_n_epoch / 2 * training_steps)\n",
    "        print('start training {} * {} model'.format(image_size, image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
