{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import random\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read  train  files:  25000\n",
      "read  test  files:  25000\n",
      "An intriguingly bold film weaves the seemingly effortless camerawork with some superb casting and an explosive soundtrack to plot the damaging effects of the crime and corruption of the Santiago underworld on 2 naive young brothers from the southern city of Temuco.Film debutant Daniella Rios is the seductive erotic dancer Gracia, working in the nightclub owned by the face of the new mini-wave in Chilean film production, Alejandro Trejo. The elder brother, played maturely by Nestor Cantillana, is easily convinced to become Trejo's lead henchman, after a night at the stripclub to celebrate younger brother Victor's (Juan Pablo Miranda) seventeenth birthday. From the establishing shot of this opening scene, the film explodes into neo-noir exploration of everything the outside world doesn't usually expect to see in this country so stereotypically conservative and catholic.Gracia's charms of seduction attract the three men like bees to honey, although the circular narrative of the three-way fantasy romance revolves around the linear portrayal of major international drug deals between Trejo's men and the 'Gringo', Eduardo Barril. Power relations become a vital theme, as society's outsiders merge in a mini-family. The prostitute holds an exotic spell over all the chilean men in the film, emerging from her ambiguous position in the periphery of society, and is seen as holding the key to all three men's futures. The relationships between Trejo and Cantillana become important, as the boys' parents are conspicious by their absence (one assumes they still live in Temuco). Therefore it is Trejo, el padrino, who 'adopts' Cantillana, and effectively 'makes him' as a man in the city. Miranda rapidly becomes the desperate outsider, as his dependency on his 'father figure', Cantillana, becomes increasingly strained by jealousy over the beautiful Gracia. However, Miranda remains trapped by the constraint of still being in school - he is dependent on Cantillana, who is dependent on Trejo, for the money to survive. Trejo, in turn, is under the thumb of the 'Gringo', and his wealth has been accumulated through drug deals and well as his strip clubs. The figure of Gracia acts as a time bomb viewed as a beautiful firework, she wraps a web of beauty inside the patriarchy but the strain can only lead to one climax.As the tensions of these power relations come to head, Gracia remains ambiguously elusive. The viewer is never sure which male figure she will commit to. The film concludes tragically and explosively in a shoot out which realigns power relations and erases half the major male protanganists. The final shot of Miranda's beaten face speeding down the PanAmericano highway is despairingly powerful. The boy has been sucked in by the lure of the city's underworld, yet has lost his only visible family, and his woman, who is his only friend in the film. He has nothing. The overriding metaphors are bold and brave. This is a gangster film in Chile. The notions of family, no sex before marriage etc, are abolished, and instead the harsh realities of the other side of Santiago's coin are displayed in all their savage glory. Trejo beats Rios brutally, Rios and Miranda make love in a cinema reel room - a whore having sex with a minor she barely knows. The 'gringos' are seen to have a financial hold over this small Latin American nation, but not through the copper mines, through the illegal path of drugs.Waissbluth's triumph is in his presentation of this dark underworld, which raises so many social questions, more perhaps than the record-breakingly successful Sexo Con Amor, within a slick, smooth firecracker of a film, which place this film firmly alongside Sexo Con Amor, Taxi Para Tres, and El chacotero Sentimental, as cinematic evidence that Chile is well and truly artistically alive and kicking in the post-transition period 15 years after the censorship of the Military Regime.\n",
      "1\n",
      "This hodge-podge adapted from a Gore Vidal novel (actually one of the great American writers) makes THE MAGIC CHRISTIAN and VALLEY OF THE DOLLS look like Fellini art-works. Raquel Welch, with an incredible body (and she's actually not very tall) in a lead role (except for KANSAS CITY BOMBER when she was quite good) playing Rex Reed's (bad movie reviewer; not critic) alter-ego, only to be surrounded by drag queen (great chick) Mae West, horny John Huston, a young and \"naive\" Farrah Fawcett (pre-Lee Majors; what a shame), and other various creep-azoids to pretend to spoof WAY too may things has nothing going for it except inter-spliced old films clips (i.e. Widmark in KISS OF DEATH, Lena Horne)...JUST so they can continue to bleed the life out of everyone.A 2 out of 10. Best performance = ?. It's so bad, it's worth seeing!\n",
      "0\n",
      "Ostensibly a story about the young child of Jimmy Stewart and Doris Day. The kid gets kidnapped to keep his parents quiet. They know something about a plot to assassinate the ambassador of an unnamed country during a performance at Albert Hall in London.The movie is rich in Hitchcockian incidents. A friendly but opaque Frenchman seems to grill the innocent Stewart -- a doctor from Indiana -- a little too intensely to be merely idly curious. Later the Frenchman shows up in Arab disguise, a knife in his back, and whispers some information about the murder plot to Stewart. Stewart tells his wife -- Doris Day looking very saucy indeed -- but refuses to cooperate with the police and risk his son's life.Instead the couple try to track down the assassins, buy them off, and get their son back, taking them from Morocco, where Hitchcock has given us his usual tourist's eye view of the customs, locations, and food, to London. There is a hilarious wild goose chase involving a set-to between Stewart and the staff of a taxidermy shop. The staff are more concerned about guarding their half-stuffed specimens than anything else, and they shuffle around protectively holding the carcasses of a leopard and a swordfish. In the course of the scuffle, Stewart manages to save his throat from being cut by the swordfish bill, but is bitten on the hand by a stuffed tiger, the action boosted along by Bernard Hermann's bumptious score. The scene ends with Stewart rushing out the door. Hitchcock ends it with a shot of a lion's head gaping at the slammed door. There is also a running gag, well done, about some visitors waiting around the couple's hotel room in London, waiting for things to be explained.There are two serious issues that are lightly touched on. One is the relationship between Stewart and Day, which is not as rosy as it ought to be, considered as a bourgeois ideal. She's been a stage musical star for some years and is internationally known. And she's given it all up to marry an ordinary guy who happens to be a doc. That's understandable in, say, a nurse or a flight attendant or almost any woman other than an international star with a promising career in her own right. It isn't delved into, but the edginess is noticeable, as it was not in the original version. It reminds me a little of an exchange between Joe Dimaggio and his then-wife Marilyn Monroe, who had just returned from entertaining the troops in Korea. \"Oh, Joe,\" she gushed, \"did you ever see ten thousand people stand up and cheer?\" \"Seventy thousand,\" muttered Joe, former hero of the New York Yankees.The second problem is one of allegiance. Who is of greater social value? One's own young son? Or an unknown ambassador. Do we put ourselves or our loved ones at risk for the sake of national stability? Day is faced with this dilemma in its starkest form at the climax in the Albert Hall. Her solution opts for allegiance to political stability, although her motives are problematic. Does she scream to save the ambassador's life, or does she do so just to release the anxiety that is overwhelming her? (Cf: Alec Guiness falling on the detonator at the end of \"The Bridge on the River Kwai.\") The photography is extremely good, and the settings can be menacing, even on a quiet street in a residential neighborhood of London. It's mid-day, and Stewart is alone and determined, but frightened too. There are footsteps echoing on Gulliver Street from someone, somewhere. Is he being followed? Is his life in danger? And where the hell is everybody who lives on this street? Hitchcock pays such close attention to location details that we can make out the garden wall bonding of the bricks beside him.The director had a rare disagreement with Francois Truffaut while being interviewed for Truffaut's otherwise laudatory book. Truffaut argued that the earlier version of \"The Man Who Knew Too Much\" lacked the depth of the later version. Hitchcock replied, \"It seems to me you want me to make films for the art house audience,\" but finally agreed that the 1930s version was the work of a talented amateur and this version was the work of a professional. No argument there.This is Hitchcock pretty much near his zenith.\n",
      "1\n",
      "I had the misfortune to see this film recently and have to sit through it. A friend purchased it for £1 and insisted we watch it as it sounded good from the story on the back cover.10 minutes into the film it was apparent that the actors were amateurs and this was an extremely low budget effort.The scenes were very poorly acted, the script was stupid and the story contained many scenes which seemed unnecessarily long, just so the movie would be of a reasonable length.For instance when the lead character rents a warehouse, the film spends a ridiculous amount of time on this scene, with meaningless dialogue which serves no real purpose or necessity to the plot.The lead actor is supposedly carrying out revenge on a woman who sleeps with guys to give them HIV, he never once thinks to get tested. Instead he turns into a crazed killer deciding to torture her before killing her and sawing her into pieces.If this sounds good and you are thinking this will have lots of gore, think again. This film has no real gory sequences and is quite tame for this type of film.It does not scare, it does not make you think, it does not offer fast paced fun. It may however put you to sleep, it is certain to bore you to tears, so please save yourself the despair and follow my heading.AVOID THIS FILM 1/2 out of 10 (this does not deserve even 1)The film was 78 minutes but seemed as if it was 2 1/2 hours.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "filepath = \"aclImdb_v1.tar.gz\"\n",
    "if not os.path.isfile(filepath):\n",
    "    result = urllib.request.urlretrieve(url, filepath)\n",
    "    print('downloaded: ', result)\n",
    "\n",
    "# 解壓縮檔案\n",
    "# Open for reading with gzip compression\n",
    "if not os.path.exists(\"data/aclImdb\"):\n",
    "    tfile = tarfile.open(\"aclImdb_v1.tar.gz\", 'r:gz')\n",
    "    result = tfile.extractall('data/')\n",
    "\n",
    "#from keras.preprocessing import sequence\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Regular expression operations\n",
    "# Compile a regular expression pattern into a regular expression object\n",
    "import re\n",
    "def rm_tags(text):\n",
    "    re_tag = re.compile(r'<[^>]+>')\n",
    "    return re_tag.sub('', text)\n",
    "import os\n",
    "def read_files(filetype):\n",
    "    path = \"data/aclImdb/\"\n",
    "    file_list = []\n",
    "    \n",
    "    positive_path = path + filetype + \"/pos/\"\n",
    "    for f in os.listdir(positive_path):\n",
    "        file_list += [positive_path + f]\n",
    "    \n",
    "    negative_path = path + filetype + \"/neg/\"\n",
    "    for f in os.listdir(negative_path):\n",
    "        file_list += [negative_path + f]\n",
    "    \n",
    "    print('read ', filetype, ' files: ', len(file_list))\n",
    "    \n",
    "    all_labels = ([1] * 12500 + [0] * 12500)\n",
    "    \n",
    "    all_texts = []\n",
    "    for fi in file_list:\n",
    "        with open(fi, encoding = 'utf8') as file_input:\n",
    "            all_texts += [rm_tags(\" \".join(file_input.readlines()))]\n",
    "    \n",
    "    return all_labels, all_texts\n",
    "\n",
    "y_train, train_text = read_files(\"train\")\n",
    "y_test, test_text = read_files(\"test\")\n",
    "print(train_text[0])\n",
    "print(y_train[0])\n",
    "print(train_text[12501])\n",
    "print(y_train[12501])\n",
    "print(test_text[0])\n",
    "print(y_test[0])\n",
    "print(test_text[12501])\n",
    "print(y_test[12501])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=\"\"\n",
    "for i in range(len(train_text)):\n",
    "    for j in range(len(train_text[i])):\n",
    "        _tmp=str(train_text[i][j])\n",
    "        if(_tmp==\" \"):\n",
    "            a+=\" \"\n",
    "        elif(str(train_text[i][j]).isalpha()==True):\n",
    "            a+=train_text[i][j]\n",
    "a=a.lower()\n",
    "a=a.split( )\n",
    "try:\n",
    "    a=a.remove([])\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "Words count: 5670799\n",
      "Unique words: 150779\n",
      "Vocabulary size: 20421\n",
      "Most common words: [('UNK', 233655), ('the', 326569), ('and', 161507), ('a', 160637), ('of', 145116), ('to', 134715), ('is', 106702), ('in', 91840), ('it', 75136), ('this', 69655)]\n",
      "atrociously\n",
      "19680\n",
      "of\n",
      "4\n",
      "country\n",
      "697\n",
      "judge\n",
      "1841\n",
      "sight\n",
      "1678\n",
      "notions\n",
      "8761\n",
      "Step 1, Average Loss= 487.0330\n",
      "Evaluation...\n",
      "\"atrociously\" nearest neighbors: silberling, rideau, piano, idyllic, introduces, approve, executives, submarine,\n",
      "\"of\" nearest neighbors: cains, nostril, traditions, pain, shrinking, sheila, thief, casper,\n",
      "\"country\" nearest neighbors: candle, mechanic, willard, wyatt, humiliated, transmitted, bust, reputations,\n",
      "\"judge\" nearest neighbors: foxy, grieco, deux, scarlet, vincenzo, archaeologist, drinking, cruz,\n",
      "\"sight\" nearest neighbors: squirrel, complication, hicks, instead, advises, recognise, slams, readings,\n",
      "\"notions\" nearest neighbors: hanged, merits, roegs, wannabes, charms, andré, macdowell, proceeded,\n",
      "Step 10000, Average Loss= 140.8132\n",
      "Step 20000, Average Loss= 53.1941\n",
      "Step 30000, Average Loss= 36.3857\n",
      "Step 40000, Average Loss= 28.2736\n",
      "Step 50000, Average Loss= 23.5987\n",
      "Step 60000, Average Loss= 20.3463\n",
      "Step 70000, Average Loss= 17.8012\n",
      "Step 80000, Average Loss= 15.9807\n",
      "Step 90000, Average Loss= 14.7599\n",
      "Step 100000, Average Loss= 14.1809\n",
      "Step 110000, Average Loss= 13.1283\n",
      "Step 120000, Average Loss= 12.2390\n",
      "Step 130000, Average Loss= 11.5000\n",
      "Step 140000, Average Loss= 10.8707\n",
      "Step 150000, Average Loss= 10.3110\n",
      "Step 160000, Average Loss= 9.9252\n",
      "Step 170000, Average Loss= 9.5688\n",
      "Step 180000, Average Loss= 9.4032\n",
      "Step 190000, Average Loss= 9.3556\n",
      "Step 200000, Average Loss= 9.0545\n",
      "Evaluation...\n",
      "\"atrociously\" nearest neighbors: silberling, piano, executives, m, introduces, rideau, idyllic, plus,\n",
      "\"of\" nearest neighbors: in, the, with, and, UNK, all, some, also,\n",
      "\"country\" nearest neighbors: UNK, scene, film, from, movie, best, other, way,\n",
      "\"judge\" nearest neighbors: food, situation, another, cast, bored, an, italian, by,\n",
      "\"sight\" nearest neighbors: instead, passion, character, mind, needed, sadly, thriller, his,\n",
      "\"notions\" nearest neighbors: anything, merits, budget, shallow, effort, young, higher, havent,\n",
      "Step 210000, Average Loss= 8.7904\n",
      "Step 220000, Average Loss= 8.5436\n",
      "Step 230000, Average Loss= 8.2106\n",
      "Step 240000, Average Loss= 8.0498\n",
      "Step 250000, Average Loss= 7.9030\n",
      "Step 260000, Average Loss= 7.7733\n",
      "Step 270000, Average Loss= 7.7987\n",
      "Step 280000, Average Loss= 7.8050\n",
      "Step 290000, Average Loss= 7.6642\n",
      "Step 300000, Average Loss= 7.5542\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.1\n",
    "batch_size = 128\n",
    "num_steps = 300000\n",
    "display_step = 10000\n",
    "eval_step = 200000\n",
    "\n",
    "# Evaluation Parameters\n",
    "eval_words = ['atrociously', 'of', 'country', 'judge', 'sight', 'notions']\n",
    "\n",
    "# Word2Vec Parameters\n",
    "embedding_size = 200 # Dimension of the embedding vector\n",
    "max_vocabulary_size = 50000 # Total number of different words in the vocabulary\n",
    "min_occurrence = 10 # Remove all words that does not appears at least n times\n",
    "skip_window = 3 # How many words to consider left and right\n",
    "num_skips = 2 # How many times to reuse an input to generate a label\n",
    "num_sampled = 64 # Number of negative examples to sample\n",
    "\n",
    "\n",
    "# Download a small chunk of Wikipedia articles collection\n",
    "url = 'http://mattmahoney.net/dc/text8.zip'\n",
    "data_path = 'text8.zip'\n",
    "if not os.path.exists(data_path):\n",
    "    print(\"Downloading the dataset... (It may take some time)\")\n",
    "    filename, _ = urllib.request.urlretrieve(url, data_path)\n",
    "    print(\"Done!\")\n",
    "# Unzip the dataset file. Text has already been processed\n",
    "#data_path = 'aclImdb_v1.tar.gz'\n",
    "\n",
    "#with zipfile.ZipFile(data_path) as f:\n",
    "#    text_words = f.read(f.namelist()[0]).lower().split()\n",
    "\n",
    "text_words=a\n",
    "\n",
    "# Build the dictionary and replace rare words with UNK token\n",
    "count = [('UNK', -1)]\n",
    "# Retrieve the most common words\n",
    "count.extend(collections.Counter(text_words).most_common(max_vocabulary_size - 1))\n",
    "# Remove samples with less than 'min_occurrence' occurrences\n",
    "for i in range(len(count) - 1, -1, -1):\n",
    "    if count[i][1] < min_occurrence:\n",
    "        count.pop(i)\n",
    "    else:\n",
    "        # The collection is ordered, so stop when 'min_occurrence' is reached\n",
    "        break\n",
    "# Compute the vocabulary size\n",
    "vocabulary_size = len(count)\n",
    "# Assign an id to each word\n",
    "word2id = dict()\n",
    "for i, (word, _)in enumerate(count):\n",
    "    #print(word)\n",
    "    #print(i)\n",
    "    word2id[word] = i\n",
    "\n",
    "data = list()\n",
    "unk_count = 0\n",
    "for word in text_words:\n",
    "    # Retrieve a word id, or assign it index 0 ('UNK') if not in dictionary\n",
    "    index = word2id.get(word, 0)\n",
    "    #print(word)\n",
    "    #print(index)\n",
    "    if index == 0:\n",
    "        unk_count += 1\n",
    "    data.append(index)\n",
    "count[0] = ('UNK', unk_count)\n",
    "id2word = dict(zip(word2id.values(), word2id.keys()))\n",
    "\n",
    "print(\"Words count:\", len(text_words))\n",
    "print(\"Unique words:\", len(set(text_words)))\n",
    "print(\"Vocabulary size:\", vocabulary_size)\n",
    "print(\"Most common words:\", count[:10])\n",
    "\n",
    "data_index = 0\n",
    "# Generate training batch for the skip-gram model\n",
    "def next_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    # get window size (words left and right + current one)\n",
    "    span = 2 * skip_window + 1\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    if data_index + span > len(data):\n",
    "        data_index = 0\n",
    "    buffer.extend(data[data_index:data_index + span])\n",
    "    data_index += span\n",
    "    for i in range(batch_size // num_skips):\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "        words_to_use = random.sample(context_words, num_skips)\n",
    "        for j, context_word in enumerate(words_to_use):\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "        if data_index == len(data):\n",
    "            buffer.extend(data[0:span])\n",
    "            data_index = span\n",
    "        else:\n",
    "            buffer.append(data[data_index])\n",
    "            data_index += 1\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "\n",
    "# Input data\n",
    "X = tf.placeholder(tf.int32, shape=[None])\n",
    "# Input label\n",
    "Y = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "\n",
    "# Ensure the following ops & var are assigned on CPU\n",
    "# (some ops are not compatible on GPU)\n",
    "with tf.device('/cpu:0'):\n",
    "    # Create the embedding variable (each row represent a word embedding vector)\n",
    "    embedding = tf.Variable(tf.random_normal([vocabulary_size, embedding_size]))\n",
    "    # Lookup the corresponding embedding vectors for each sample in X\n",
    "    X_embed = tf.nn.embedding_lookup(embedding, X)\n",
    "\n",
    "    # Construct the variables for the NCE loss\n",
    "    nce_weights = tf.Variable(tf.random_normal([vocabulary_size, embedding_size]))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "# Compute the average NCE loss for the batch\n",
    "loss_op = tf.reduce_mean(\n",
    "    tf.nn.nce_loss(weights=nce_weights,\n",
    "                   biases=nce_biases,\n",
    "                   labels=Y,\n",
    "                   inputs=X_embed,\n",
    "                   num_sampled=num_sampled,\n",
    "                   num_classes=vocabulary_size))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluation\n",
    "# Compute the cosine similarity between input data embedding and every embedding vectors\n",
    "X_embed_norm = X_embed / tf.sqrt(tf.reduce_sum(tf.square(X_embed)))\n",
    "embedding_norm = embedding / tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keepdims=True))\n",
    "cosine_sim_op = tf.matmul(X_embed_norm, embedding_norm, transpose_b=True)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    # Testing data\n",
    "    for w in eval_words:\n",
    "        print(w)\n",
    "        bytesW = w\n",
    "        #i = word2id.get(bytesW, 0)\n",
    "        i = word2id[w]\n",
    "        print(i)\n",
    "\n",
    "    x_test = np.array([word2id[w] for w in eval_words])\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in range(1, num_steps + 1):\n",
    "        # Get a new batch of data\n",
    "        batch_x, batch_y = next_batch(batch_size, num_skips, skip_window)\n",
    "        # Run training op\n",
    "        _, loss = sess.run([train_op, loss_op], feed_dict={X: batch_x, Y: batch_y})\n",
    "        average_loss += loss\n",
    "\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            if step > 1:\n",
    "                average_loss /= display_step\n",
    "            print(\"Step \" + str(step) + \", Average Loss= \" + \\\n",
    "                  \"{:.4f}\".format(average_loss))\n",
    "            average_loss = 0\n",
    "\n",
    "        # Evaluation\n",
    "        if step % eval_step == 0 or step == 1:\n",
    "            print(\"Evaluation...\")\n",
    "            sim = sess.run(cosine_sim_op, feed_dict={X: x_test})\n",
    "            for i in range(len(eval_words)):\n",
    "                top_k = 8  # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = '\"%s\" nearest neighbors:' % eval_words[i]\n",
    "                for k in range(top_k):\n",
    "                    log_str = '%s %s,' % (log_str, id2word[nearest[k]])\n",
    "                print(log_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
